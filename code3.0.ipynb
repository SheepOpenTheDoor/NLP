{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS918 Exercise One\n",
    "    Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk, json, time, operator, copy\n",
    "from nltk.stem import *\n",
    "from collections import Counter\n",
    "\n",
    "start = time.time()\n",
    "f = open(\"signal-news1.jsonl\", \"r+\")\n",
    "\n",
    "#read all the dictionaries in the jsonl file then store them in list 'dict'\n",
    "dic = []\n",
    "for line in f:\n",
    "    dic.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A1: Text preprocessing\n",
    "    Read the values of \"content\" in every dictionary then lower the characters\n",
    "   ### Q1.4: Remove URLs. Note that URLs may appear in different forms\n",
    "       Using Regular Expression \"(http|ftp|https):\\/\\/[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?\"\n",
    "   ### Q1.1: Remove all non-alphanumeric characters except spaces\n",
    "       Using Regular Expression \"[^a-z0-9 ]\"\n",
    "   ### Q1.2: Remove words with 3 characters or fewer\n",
    "       Using Regular Expression \"\\b[a-z]{0,1}\\b\"\n",
    "   ### Q1.3: Remove numbers that are fully made of digits\n",
    "       Using Regular Expression \"[^a-z][0-9]+[^a-z]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "ques1 = []\n",
    "for read in dic:\n",
    "    ques1.append(read[\"content\"].lower())\n",
    "    ques1[i] = re.sub(r'(http|ftp|https):\\/\\/[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?', '', ques1[i])  #Q1.4\n",
    "    ques1[i] = re.sub(r'[^a-z0-9 ]', '', ques1[i])  #Q1.1\n",
    "    ques1[i] = re.sub(r'\\b[a-z]{0,1}\\b', '',ques1[i])  #Q1.2\n",
    "    ques1[i] = re.sub(r'[^a-z][0-9]+[^a-z]', ' ', ques1[i])  #Q1.3\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A2\n",
    "   ### Q2.1: Use an English lemmatiser to process all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "buff = []  #store the consequence of each word after lemmatizing\n",
    "ques5 = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for read in ques1:\n",
    "    ques5.append(read)\n",
    "    ques5[i] = nltk.word_tokenize(ques5[i])\n",
    "    #find the tags for each words then lemmatize the word with the tag\n",
    "    for words in ques5[i]:\n",
    "        #using default pos tag 'n'\n",
    "        buff.append(wordnet_lemmatizer.lemmatize(words, 'n'))\n",
    "        '''#using pos tag\n",
    "        tags = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "        tag = tags[0][1]\n",
    "        if tag == 'NN' or tag == 'NNS' or tag == 'NNP' or tag == 'NNPS':\n",
    "            buff.append(wordnet_lemmatizer.lemmatize(words, pos = 'n'))\n",
    "        elif tag == 'VB' or tag == 'VBD' or tag == 'VBG' or tag == 'VBN' or tag == 'VBP' or tag == 'VBZ':\n",
    "            buff.append(wordnet_lemmatizer.lemmatize(words, pos = 'v'))\n",
    "        elif tag == 'JJ' or tag == 'JJR' or tag == 'JJS':\n",
    "            buff.append(wordnet_lemmatizer.lemmatize(words, pos = 'a'))\n",
    "        elif tag == 'R' or tag == 'RB' or tag == 'RBR' or tag == 'RBS' or tag == 'IN':\n",
    "            buff.append(wordnet_lemmatizer.lemmatize(words, pos = 'r'))\n",
    "        else:\n",
    "            #buff.append(wordnet_lemmatizer.lemmatize(words, pos = 'v'))\n",
    "            buff.append(words)'''\n",
    "    ques5[i] = copy.copy(buff)\n",
    "    buff = []\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: N-grams\n",
    "   ### Q1.1: Compute N (number of tokens) and V (vocabulary size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens is 5692991, Vocabulary size is 123539.\n"
     ]
    }
   ],
   "source": [
    "content = []  #using for reading all the words in a one-dimension list\n",
    "for item in ques5:\n",
    "    for word in item:\n",
    "        content.append(word)\n",
    "#ques6 = nltk.FreqDist(content)\n",
    "ques6 = Counter(content)\n",
    "\n",
    "V = len(ques6)\n",
    "N = len(content)\n",
    "\n",
    "#Q1.1 consequence\n",
    "print('Number of tokens is %d, Vocabulary size is %d.'%(N, V))\n",
    "#print(ques6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Q1.2: List the top 25 trigrams based on the number of occurrences on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ranking': 1, 'trigrams': ('one', 'of', 'the'), 'times': 2428}\n",
      "{'ranking': 2, 'trigrams': ('on', 'share', 'of'), 'times': 2093}\n",
      "{'ranking': 3, 'trigrams': ('day', 'moving', 'average'), 'times': 1979}\n",
      "{'ranking': 4, 'trigrams': ('on', 'the', 'stock'), 'times': 1566}\n",
      "{'ranking': 5, 'trigrams': ('in', 'research', 'report'), 'times': 1422}\n",
      "{'ranking': 6, 'trigrams': ('a', 'well', 'a'), 'times': 1421}\n",
      "{'ranking': 7, 'trigrams': ('in', 'research', 'note'), 'times': 1373}\n",
      "{'ranking': 8, 'trigrams': ('for', 'the', 'quarter'), 'times': 1221}\n",
      "{'ranking': 9, 'trigrams': ('the', 'united', 'state'), 'times': 1216}\n",
      "{'ranking': 10, 'trigrams': ('average', 'price', 'of'), 'times': 1193}\n",
      "{'ranking': 11, 'trigrams': ('research', 'report', 'on'), 'times': 1177}\n",
      "{'ranking': 12, 'trigrams': ('research', 'note', 'on'), 'times': 1138}\n",
      "{'ranking': 13, 'trigrams': ('share', 'of', 'the'), 'times': 1132}\n",
      "{'ranking': 14, 'trigrams': ('the', 'end', 'of'), 'times': 1129}\n",
      "{'ranking': 15, 'trigrams': ('in', 'report', 'on'), 'times': 1124}\n",
      "{'ranking': 16, 'trigrams': ('earnings', 'per', 'share'), 'times': 1119}\n",
      "{'ranking': 17, 'trigrams': ('cell', 'phone', 'plan'), 'times': 1073}\n",
      "{'ranking': 18, 'trigrams': ('phone', 'plan', 'detail'), 'times': 1070}\n",
      "{'ranking': 19, 'trigrams': ('of', 'the', 'company'), 'times': 1057}\n",
      "{'ranking': 20, 'trigrams': ('according', 'to', 'the'), 'times': 1045}\n",
      "{'ranking': 21, 'trigrams': ('buy', 'rating', 'to'), 'times': 1016}\n",
      "{'ranking': 22, 'trigrams': ('appeared', 'first', 'on'), 'times': 995}\n",
      "{'ranking': 23, 'trigrams': ('moving', 'average', 'price'), 'times': 995}\n",
      "{'ranking': 24, 'trigrams': ('price', 'target', 'on'), 'times': 981}\n",
      "{'ranking': 25, 'trigrams': ('part', 'of', 'the'), 'times': 933}\n"
     ]
    }
   ],
   "source": [
    "trigrams = []  #store all the trigrams\n",
    "for i in range(len(ques5)):\n",
    "    for j in range(len(ques5[i]) - 2):\n",
    "        trigrams.append((ques5[i][j], ques5[i][j + 1], ques5[i][j + 2]))\n",
    "count = Counter(trigrams)\n",
    "count_list = []  #read the trigrams and the times they happend then store them as a list of dictionary\n",
    "for k, v in count.items():\n",
    "    count_list.append({'trigrams': k, 'times': v})\n",
    "count_list = sorted(count_list, key = operator.itemgetter('times'), reverse = True)\n",
    "ques7 = []  #output the top 25 occurrent trigrams\n",
    "i = 1\n",
    "for item in count_list:\n",
    "    if i < 26:\n",
    "        ques7.append({'ranking': i, 'trigrams': item['trigrams'], 'times': item['times']})\n",
    "        i += 1\n",
    "    else:\n",
    "        break\n",
    "for item in ques7:\n",
    "    print(item)  #Q1.2 consequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Q1.3:  Compute the number of positive and negative word counts in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive word in the corpus is 170511.\n",
      "The number of negative word in the corpus is 129552.\n"
     ]
    }
   ],
   "source": [
    "f_positive = open('positive-words.txt', 'r')\n",
    "f_negative = open('negative-words.txt', 'r')\n",
    "pos_word = []\n",
    "neg_word =[]\n",
    "\n",
    "total_content = []\n",
    "for k, v in ques6.items():\n",
    "    total_content.append(k)\n",
    "\n",
    "#Compute the number of positive words\n",
    "poscount = 0\n",
    "for posword in f_positive:\n",
    "    pos_word.append(posword.strip())\n",
    "posinter = list(set(total_content)&set(pos_word))   #compute the union set of total content and the positive words\n",
    "for k, v in ques6.items():\n",
    "    for word in posinter:\n",
    "        if word == k:\n",
    "            poscount += v\n",
    "\n",
    "#Compute the number of negative words\n",
    "negcount = 0\n",
    "for negword in f_negative:\n",
    "    neg_word.append(negword.strip())\n",
    "neginter = set(total_content)&set(neg_word)  #compute the union set of total content and the negative words\n",
    "for k, v in ques6.items():\n",
    "    for word in neginter:\n",
    "        if word == k:\n",
    "            negcount += v\n",
    "\n",
    "#Q1.3 consequence\n",
    "print('The number of positive word in the corpus is %d.'%(poscount))\n",
    "print('The number of negative word in the corpus is %d.'%(negcount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Q1.4.1: Compute the number of news stories with more positive than negative words\n",
    " ### Q1.4.2: Compute the number of news stories with more negative than positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stories with more positive than negative words is:  10635\n",
      "The number of stories with more negative than positive words is:  6086\n",
      "The number of stories with a tie is:  2507\n"
     ]
    }
   ],
   "source": [
    "morepos = 0  #count the number of the stories which have more positive words than negative words\n",
    "moreneg = 0  #count the number of the stories which have more negative words than positive words\n",
    "tie = 0  #count the number of the stories which have equal number of negative words and positive words\n",
    "\n",
    "line_content = []\n",
    "for line in ques5:\n",
    "    poscount1 = 0\n",
    "    negcount1 = 0\n",
    "    count1 = Counter(line)\n",
    "    for k, v in count1.items():\n",
    "        line_content.append(k)\n",
    "    \n",
    "    posinter1 = set(line_content)&set(pos_word)\n",
    "    for k, v in count1.items():\n",
    "        for word in posinter1:\n",
    "            if word == k:\n",
    "                poscount1 += 1\n",
    "    \n",
    "    neginter1 = set(line_content)&set(neg_word)\n",
    "    for k, v in count1.items():\n",
    "        for word in neginter1:\n",
    "            if word == k:\n",
    "                negcount1 += 1\n",
    "    \n",
    "    if poscount1 > negcount1:\n",
    "        morepos += 1\n",
    "    elif poscount1 < negcount1:\n",
    "        moreneg += 1\n",
    "    else:\n",
    "        tie += 1\n",
    "    \n",
    "    line_content = []\n",
    "\n",
    "#Q1.4 consequence\n",
    "print('The number of stories with more positive than negative words is: ', morepos)\n",
    "print('The number of stories with more negative than positive words is: ', moreneg)\n",
    "print('The number of stories with a tie is: ', tie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part C\n",
    "     Since all the \"is\" are lemmatized to \"be\" after lemmatization, so the corpus has been chosen in part C is the one before lemmatization.\n",
    "  ### Q1.1: Compute language models for trigrams based on the first 16,000 rows of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3010683 trigrams based on first 16000 rows of the corpus.\n"
     ]
    }
   ],
   "source": [
    "trigrams16 = []  #store all the trigrams in first 16000 rows of the corpus\n",
    "\n",
    "q4content = []  #store all the words after the preprocessing in the partA1\n",
    "x = 0\n",
    "for read in ques1:\n",
    "    q4content.append(read)\n",
    "    q4content[x] = nltk.word_tokenize(q4content[x])\n",
    "    x += 1\n",
    "\n",
    "for i in range(16000):\n",
    "    for j in range(len(q4content[i]) - 2):\n",
    "        trigrams16.append((q4content[i][j], q4content[i][j + 1], q4content[i][j + 2]))\n",
    "count16 = Counter(trigrams16)\n",
    "count16_list = []  #read the trigrams and the times they happend then store them as a list of dictionary\n",
    "for k, v in count16.items():\n",
    "    count16_list.append({'trigrams': k, 'times': v})\n",
    "print('There are %d trigrams based on first 16000 rows of the corpus.'%len(count16_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Q1.2: Beginning with the bigram “is this”, produce a sentence of 10 words by appending the most likely next word each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'this', 'the', 'company', 'has', 'market', 'capitalization', 'of', 'billion', 'and']\n"
     ]
    }
   ],
   "source": [
    "times = 0\n",
    "char = []\n",
    "char.append('is')\n",
    "char.append('this')\n",
    "i = 0\n",
    "for j in range(8):\n",
    "    char.append(' ')\n",
    "    for item in count16_list:\n",
    "        if item['trigrams'][0] == char[i] and item['trigrams'][1] == char[i + 1]:\n",
    "            #find the highest occurence of the trigram which contain the bigram at first\n",
    "            if item['times'] > times:\n",
    "                times = item['times']\n",
    "                char[i+2] = item['trigrams'][2]\n",
    "    i += 1\n",
    "    times = 0\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Q2: Compute the perplexity by evaluating on the remaining rows of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1786.573617967546\n"
     ]
    }
   ],
   "source": [
    "recontent = []  #store the words in remianing rows of the corpus as a list of lists\n",
    "reword = []  #store the words in remianing rows of the corpus as a list\n",
    "i = 0\n",
    "for line in ques1:\n",
    "    recontent.append(line)\n",
    "    recontent[i] = nltk.word_tokenize(recontent[i])\n",
    "    i += 1\n",
    "for i in range(16001, 19228):\n",
    "    for word in recontent[i]:\n",
    "        reword.append(word)\n",
    "V16 = len(Counter(reword))   #vocabulary of the 16000+ rows\n",
    "\n",
    "trigram = []\n",
    "for i in range(16001, 19228):\n",
    "    for j in range(len(recontent[i]) - 2):\n",
    "        trigram.append((recontent[i][j], recontent[i][j + 1], recontent[i][j + 2]))\n",
    "\n",
    "bigram = []\n",
    "for i in range(16001, 19228):\n",
    "    for j in range(len(recontent[i]) - 1):\n",
    "        bigram.append((recontent[i][j], recontent[i][j + 1]))\n",
    "\n",
    "def conprop(char1, char2, char3):\n",
    "    cprop = 0\n",
    "    ABCcount = 1  #smooth the data by add 1\n",
    "    ABcount = V16  #smooth the data by add V(vocabulary size) after 16000 row\n",
    "    ABClist = [char1, char2, char3]\n",
    "    ABlist = [char1, char2]\n",
    "    for item in trigram:\n",
    "        if ABClist[0] == item[0] and ABClist[1] == item[1] and ABClist[2] == item[2]: \n",
    "            ABCcount += 1\n",
    "    for item in bigram:\n",
    "        if ABlist[0] == item[0] and ABlist[1] == item[1]:\n",
    "            ABcount += 1\n",
    "    cprop = float(ABCcount/ABcount)\n",
    "    return cprop\n",
    " \n",
    "propsum = 1\n",
    "for i in range(8):\n",
    "    propsum *= conprop(char[i], char[i + 1], char[i + 2])\n",
    "\n",
    "pp = pow(propsum, -1/8)\n",
    "print(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Compute the time running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479.46166372299194\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
